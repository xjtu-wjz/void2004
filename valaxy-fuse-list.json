[{"title":"2024的简单小结","tags":["人生"],"categories":"生活","author":"void","excerpt":"\n不知不觉间2024结束了，这一年确实经历了很多很多事，本来想说的有千言万语，但真正涌到嘴边的却寥寥无几，最大的感触也许就是--过去的生活多少有些狭隘了，狭隘的生活不会提供有趣的阅历，也不会帮助我在科","link":"/posts/2024--%E5%B0%8F%E7%BB%93"},{"title":"Paper reading--《Denoising Diffusion Implicit Models》","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\n\n","link":"/posts/DENOISING%20DIFFUSION%20IMPLICIT%20MODELS"},{"title":"Paper reading--《Denoising Diffusion Probabilistic Models》","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\n扩散模型的开山之作[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。这篇帖子并不把重心放在详细","link":"/posts/Denoising%20Diffusion%20Probabilistic%20Models"},{"title":"使用Determined-AI 的bug集散地","tags":["科研","Tech"],"categories":"Tech","author":"void","excerpt":"\n组里使用Determined AI管理集群，比原来使用conda管理环境麻烦超级多，但是也规范很多。尽管学长们写了很多文档了，但对于一个新上手的小白来说要想快速学会如何创建环境然后跑自己的代码还是非","link":"/posts/Determined%20AI%E7%9A%84bug%E9%9B%86%E6%95%A3%E5%9C%B0"},{"title":"Paper reading--classifier guidance and classifier-free guidance","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"两种改进diffusion model的方法，思路相反，分别利用和去除classifier guidance.首先我们看带有分类器指引的，再看不带分类器指引的。\n\n尽管最近的diffusion mod","link":"/posts/Diffusion%20Models%20Beat%20GANs%20on%20Image%20Synthesis"},{"title":"ECML-PKDD 2025投稿日记","tags":["科研","PUL","paper-posting"],"categories":"Research","author":"void","excerpt":"# Preliminary\n\n此帖用于记录赶稿ECML-PKDD2025的具体工作细节和心路历程。如果中了，希望能给后面投稿的学弟学妹一些启示与参考；如果没有的话（好像大概率中不了hhh）也无所谓，发","link":"/posts/ECML-PKDD%202025%E6%8A%95%E7%A8%BF%E6%97%A5%E8%AE%B0"},{"title":"Paper reading--《EduChat-A Large-Scale Language Model-based Chatbot System for Intelligent Education》","tags":["科研","LLM"],"categories":"Paper-reading","author":"void","excerpt":"华东师范大学提出的一款面向基础教育的苏格拉底式大模型。\n\n# 教育大模型面对的问题\n在多类领域LLMs已经取得了瞩目的成就，但是在教育领域LLM的表现依然欠佳，主要有如下原因：\n- 大模型基于基础数据","link":"/posts/EduChat%20_A%20Large%20Scale%20Language%20Model%20based%20Chatbot%20System%20%20for%20Intelligent%20Education"},{"title":"Paper reading--《Efficiency for Free--Ideal Data Are Transportable Representations》","tags":["科研","DCAI"],"categories":"Paper-reading","author":"void","excerpt":"\n文章提出，使用一个公开的，并不针对数据集或者特定的下流任务进行特化的模型已经可以产生高质量的训练数据，并且这些数据可以有效帮助加速模型训练过程和提升训练性能。基于这个发现，提出了Representa","link":"/posts/Efficiency%20for%20Free--Ideal%20Data%20Are%20%20Transportable%20Representations"},{"title":"Paper reading--《Energy-based Out-of-distribution Detection》","tags":["科研","OOD"],"categories":"Paper-reading","author":"void","excerpt":"\n# 能量分数概述\n\n首先什么是能量分数方法介绍：\n\n一个模型区分OOD和ID数据的主要依据是它们的能量分数，这个就是energy-based方法。\n\n大致流程是这样：\n\n![alt text](..","link":"/posts/Energy-based%20Out-of-distribution%20Detection"},{"title":"Paper reading--《High-Resolution Image Synthesis with Latent Diffusion Models》","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\nStable Diffusion 是当前最受欢迎的Image generative model，主要原因就是SD model极大提升了diffusion model生成图像的效率，令用户可以在自己的","link":"/posts/High-Resolution%20Image%20Synthesis%20with%20Latent%20Diffusion%20Models"},{"title":"ICML 2025投稿日记","tags":["科研","Generative Models","paper-posting"],"categories":"Research","author":"void","excerpt":"\n# 11.11 确定方向\n特殊的日子，铝坨坨wave75 到了，同时也确定了具体做做加速推理以及高质量生成方向的工作，标杆是DDIM的论文，希望能提出像他们一样简单而优雅的方法。\n\n# 11.27 ","link":"/posts/ICML%202025%E6%8A%95%E7%A8%BF%E6%97%A5%E8%AE%B0"},{"title":"Paper reading--<<Instruct, Not Assist LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging>>","tags":["科研","LLM"],"categories":"Paper-reading","author":"void","excerpt":"\n# 苏格拉底多轮教学引导\n现有LLMs习惯于直接针对提出的问题给出答案，但这并不是一种优秀的教学方式。因此本文提出将苏格拉底式引导用于大模型教学，特殊地，处理引导学生对出错代码进行debug。\n\n#","link":"/posts/Instruct,%20Not%20Assist-%20%20LLM-based%20Multi-Turn%20Planning%20and%20Hierarchical%20Questioning%20%20for%20Socratic%20Code%20Debugging"},{"title":"Paper reading--《Learning From Positive and Unlabeled Data-A Survey》","tags":["科研","PUL"],"categories":"Paper-reading","author":"void","excerpt":"\n\n论文通篇简述了PU-learning几个经典的问题，并且详细介绍了PUL到底是什么。\n\n# PU-learning与传统半监督学习\n\nPU-learning是传统半监督学习的变种，与传统半监督学习","link":"/posts/Learning%20From%20Positive%20and%20Unlabeled%20Data-A%20Survey"},{"title":"Paper reading--《Learning Transferable Visual Models From Natural Language Supervision》","tags":["科研","Generative model"],"categories":"Paper-reading","author":"void","excerpt":"\n一句话概括，clip介绍了一种将自然语言监督引入计算机视觉领域的方法（或者说，一类新的encoder）。为后面包括LDM等工作使用自然语言作为约束引入图像生成提供了启发。\n\n# Abstract &","link":"/posts/Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision"},{"title":"Paper reading--《OOD-intro》","tags":["OOD","科研"],"categories":"Paper-reading","author":"void","excerpt":"# OOD简介\nOOD（out of distribution detection）是异常检测（Anomaly Detection）领域的一个重要分支，通俗来讲OOD检测的任务就是**识别出分布与模型","link":"/posts/OOD-intro"},{"title":"Paper reading--扩散模型推理与采样加速","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\n# The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling\n正式开始之前首先我们需要先学习目前有很多ODE-based 扩","link":"/posts/Paper%20reading-%E5%8A%A0%E9%80%9F%E6%8E%A8%E7%90%86%E5%92%8C%E9%AB%98%E8%B4%A8%E9%87%8F%E6%B3%9B%E5%8C%96"},{"title":"Paper reading--Scalable Diffusion Models with Transformers","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\n文章提出了经典的diffusion transformer。Dit的核心思想是使用transformer代替原本在扩散模型中负责预测噪声的U-net。使用transformer之后的扩散模型可以有效","link":"/posts/Scalable%20Diffusion%20Models%20with%20Transformers"},{"title":"Paper reading--《Self-PU Self Boosted and Calibrated Positive-Unlabeled Training》","tags":["PUL","科研"],"categories":"Paper-reading","author":"void","excerpt":"# self-PU\n\n文章提出了一种新颖的PUL问题框架。先前的PUL方法集中精力于损失敏感上面，直接将未标记样本作为负样本进行损失优化，然后对于未标记样本中各个样本赋予不同权重进行学习。问题也很明显","link":"/posts/Self-PU_Self%20Boosted%20and%20Calibrated%20Positive-Unlabeled%20Training"},{"title":"WWW 2025投稿日记","tags":["科研","PUL","paper-posting"],"categories":"Research","author":"void","excerpt":"","link":"/posts/WWW%202025%E6%8A%95%E7%A8%BF%E6%97%A5%E8%AE%B0"},{"title":"Paper reading--《Attention is all you need》","tags":["科研","LLM"],"categories":"Paper-reading","author":"void","excerpt":"\n大模型的根基--大名鼎鼎的transformer降世的论文。原文链接：[Attention Is All You Need](https://arxiv.org/abs/1706.03762)。\n\n","link":"/posts/attention%20is%20all%20you%20need"},{"title":"Paper reading--《使用野生数据集检测OOD》","tags":["OOD","科研"],"categories":"Paper-reading","author":"void","excerpt":"\n\n使用无标签数据预测OOD--HOW DOES UNLABELED DATA PROVABLY HELP OUT-OF-DISTRIBUTION DETECTION\n\n原文链接：[2402.0350","link":"/posts/%E4%BD%BF%E7%94%A8%E9%87%8E%E7%94%9F%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A3%80%E6%B5%8BOOD"},{"title":"周记 week1 (8.26~9.1)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n## 关于这个网站\n\n开始写的第一篇周记，其实博客很早就建好了只是一直没有往上面更新东西。voidmyworld存在的重要原因一方面是我希望能找到一个形式更加自由的空间存我想写的东西。身为一个攻击性","link":"/posts/%E5%91%A8%E8%AE%B0%20week1%20(8.26~9.1)"},{"title":"周记 week10 (11.11~11.17)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n# 生日快乐\n\n这周终于到过生日的时候了2333, 但是非常不幸地撞上了数据挖掘考试，还得感慨下为什么从小学到本科貌似生日经常考试，不能理解。\n\n因为要考试，所以不得不跟学长请假了，组里的事情暂时得","link":"/posts/%E5%91%A8%E8%AE%B0%20week10%20(11.11~11.17)"},{"title":"周记 week11 (11.18~11.24)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n# 学业\n第一个考试季结束了！！！\n\n目前来看凸优化和数据挖掘的两门都在掌控之内，分数可能不会逆天的高，但是都上90，运气好的话冲冲满绩还是问题不大的。凸优化给我的感觉还是非常偏应试那一套，优化方法","link":"/posts/%E5%91%A8%E8%AE%B0%20week11%20(11.18~11.24)"},{"title":"周记 week12 (11.25~12.01)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n\n暂时结束考试，回归科研之后的第一周吧。\n\n# 学业\n因为刚从考试周回来，状态有少许下滑，因此几篇经典文献又重新读了下。接着往这个采样和训练加速的方向又读了几篇，不禁感慨下generative mo","link":"/posts/%E5%91%A8%E8%AE%B0%20week12%20(11.25~12.1)"},{"title":"周记 week13~16 (12.02~1.05)","tags":["周记","科研","旅欧日记"],"categories":"生活","author":"void","excerpt":"\n很久没有更周记了，12月到12月中旬停止的原因是因为刚刚拿到集群了，中间的适应过程还是折磨了一会，用熟了之后必须感叹一下这种数据和代码分开管理，环境独立维护的集群服务对于很吃算力资源的方向真的非常有","link":"/posts/%E5%91%A8%E8%AE%B0%20week13~16(12.2~1.5)"},{"title":"周记 week2 (9.2~9.8)","tags":["周记"],"categories":"生活","author":"void","excerpt":"\n# 这一周\n百无聊赖的一周捏，然后回了学校。科研没咋做，趁着开学前最后好好玩了玩，开学后也许有更多故事可以说说。","link":"/posts/%E5%91%A8%E8%AE%B0%20week2%20(9.2~9.8)"},{"title":"周记 week3 (9.9~9.15)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n开学的第一个星期(*´▽｀)ノノ，精神状态明显比上学期好多了。课内成绩基本上再也不用花太多时间精力，重心可以放在科研，每天的生活不像大一大二那样兵荒马乱了，科研看书打球打游戏，安排的明明白白。\n\n#","link":"/posts/%E5%91%A8%E8%AE%B0%20week3%20(9.9~9.15)"},{"title":"周记 week4 (9.16~9.22)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n平稳的一周，一边推进论文一边过日子，中间放了个中秋的假，好玩的事其实也不少，当然糟心的事也有。\n\n# 钓鱼\n跟罗神他们一起六个人跑去钓鱼，着实二十岁的年纪，三十岁的身体，四十岁的爱好了。深刻感受到钓","link":"/posts/%E5%91%A8%E8%AE%B0%20week4%20(9.16~9.22)"},{"title":"周记 week5 (10.7~10.13)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n# 科研\n国庆赶了很长时间的稿，终于赶在12号dll之前交上了论文。滑稽的是竟然CMT还崩了，中间着急忙慌写了封邮件给chair看看是怎么回事，网站的问题能不能就再给宽限一段时间。不得不说chair","link":"/posts/%E5%91%A8%E8%AE%B0%20week5%20(10.7~10.13)"},{"title":"周记 week6&7 (10.14~10.27)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n上周是截稿之后的第一周，轻轻松松地放松了一星期，也正好一星期确实无事发生。上一周倒是事情特别多，索性将两周周记合并。\n\n# 学业\n截稿啦(*´▽｀)ノノ 出人意料的PKDD投稿之前系统还炸了一次，给","link":"/posts/%E5%91%A8%E8%AE%B0%20week6&7%20(10.14~10.27)"},{"title":"周记 week8 (10.28~11.03)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n# 学业/科研\n计网和数据挖掘大作业撞在了一周，忙的真的想抑郁。计网作业这边，读的论文和其他LLMs for network相比更像是一个tech report。最终组织内容和复现代码的任务基本80","link":"/posts/%E5%91%A8%E8%AE%B0%20week8%20(10.28~11.3)"},{"title":"周记 week9 (11.04~11.10)","tags":["周记","科研"],"categories":"生活","author":"void","excerpt":"\n# 学业\n按理来说，其实也就是一个平平无奇的平淡的一周。中间没有什么大事，但是琐碎的小事不少。\n\n值得一提的就是新的方向终于确定了。通过一段时间的调研还是最终联系决定做生成模型方向的研究了。一篇Ge","link":"/posts/%E5%91%A8%E8%AE%B0%20week9%20(11.4~11.10)"},{"title":"深度学习基础概念","tags":["科研"],"categories":"Research","author":"void","excerpt":"\n近几年深度学习领域的基础概念和模型基础。\n\n# 残差连接\n在传统神经网络中，信息会从一个层传递到下一个层，在传递的过程中会损失部分信息。在残差连接技术中，在原本layer的输出基础上又加了一个跨层的","link":"/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-basic%20concept"},{"title":"班会感想（补档）","tags":["科研","锐评"],"categories":"生活","author":"void","excerpt":"\n这篇长文其实是前几天班会开完之后的写的，当时直接发在空间里，还是引起了不少同学的共鸣。现在把他搬过来。\n\n原文：\n\n吐槽/谈本科生进组科研/谈大三上进组\n（全文很长，而且有些话不吐不快，攻击性会非常","link":"/posts/%E7%8F%AD%E4%BC%9A%E6%84%9F%E6%83%B3%EF%BC%88%E8%A1%A5%E6%A1%A3%EF%BC%89"},{"title":"Paper reading--生成模型训练加速","tags":["科研","Generative Models"],"categories":"Paper-reading","author":"void","excerpt":"\n\n# REPRESENTATION ALIGNMENT FOR GENERATION: TRAINING DIFFUSION TRANSFORMERS IS EASIER THAN YOU THIN","link":"/posts/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8A%A0%E9%80%9F"},{"title":"歌声と夏--蝉鸣中的旋律","tags":["生活"],"categories":"生活","author":"void","excerpt":"\n对日语歌一直有着特殊的偏好，从俳句开始就比较中意日语效率低但是句式偏工整的表达习惯，和隐藏于每一个助词，每一个语气词的中二感。\n\n最喜欢的几首旋律差别不小，但是普遍都是以summer(盛夏)或者ch","link":"/posts/%E7%A5%9E%E7%A7%98%E8%AE%B0%E5%BD%95"}]
