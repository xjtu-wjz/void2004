---
title: Paper reading--《Efficiency for Free--Ideal Data Are Transportable Representations》
date: 2024-11-06
updated: 2023-11-06
categories: Paper-reading
image: https://raw.githubusercontent.com/xjtu-wjz/void2004/refs/heads/main/pics_for_post/100468452_p0.webp
tags:
  - 科研
  - LLM
top: 1
---

文章提出，使用一个公开的，并不针对数据集或者特定的下流任务进行特化的模型已经可以产生高质量的训练数据，并且这些数据可以有效帮助加速模型训练过程和提升训练性能。基于这个发现，提出了Representation Learning Accelerator（RELA）框架，使用公开的模型生产高质量训练数据，并增加自监督学习策略提升训练效果和训练速度。

# Intro
随着近年来深度学习领域数据量的爆发式增长，随之而来的还有若干重要训练问题：
- 当前的深度学习模型普遍需要人工构造高质量标注数据集，而人工标注过程会消耗大量社会资源。
- 使用越来越大的数据训练模型，所需要消耗的计算资源也越来越多。

为了解决以上问题，研究者先后引入了自监督学习（SSL）和数据集蒸馏方法。前者无需人工标注数据，后者大大压缩了原始数据集。但这些方法也存在着自己的局限性，导致原来的问题并没有完全解决的同时反而引入了新的挑战：
- 自监督学习产生的标签是次优化的，训练效率不如利用人工标签那样高效；
- 数据集蒸馏的开销非常大，可能将数据集蒸馏的过程消耗和不进行蒸馏直接训练多出的消耗相当。

对此文章提出了一个崭新的视角--网络上存在大量的预训练好的模型，它们可以被视作已经存放了部分知识。利用这些先验模型也许可以帮助我们提升自己的模型训练效率。但值得注意的是，网上的预训练模型并不针对特定的下游任务和数据集。因此又提出open problem:我们该如何才能利用这些不限定数据集和任务的先验模型来加速针对某一特定下游任务的模型的训练效率？于是RELA应运而生。

贡献分为五方面：
- 揭示了数据的哪些特性可以提升/降低训练效率。
- 从data-centric视角揭示了哪些因素会造成了自监督学习的低效率问题。
- 提供了一套量化在优化数据集上训练的模型的泛化能力的方法。
- 提出RELA方法，能够产生加速模型训练的高质量数据集，并且确保模型在产生数据集上的泛化性能。
- 将RELA方法应用于众多任务场景，网络架构和数据类型，在相同甚至更少的预算的前提下取得了相同甚至更优的性能。

# 能够提升训练效率的数据属性
#### 从数据驱动的视角统一监督学习和自监督学习
传统自监督学习和监督学习其实都能看成是对从样本空间$D_X$到目标空间$D_Y$的映射关系的学习。两种学习范式种都需要生成目标：
$$( D_Y = \{y \mid y = \psi(x) \text{ s.t. } x \sim D_X\} )$$

只不过在监督学习中，目标空间是由人工手动标注的，在训练过程中不变；而自监督学习范式中的标签，例如在BYOL方法中，它使用学习模型 $(\phi_\theta)$ 的指数移动平均（Exponential Moving Average, EMA）版本来动态生成目标 $(y = \text{EMA}[\phi_\theta](x))$。这些目标随着模型 $(\phi_\theta)$ 的不断更新而动态变化。

既然都是从输入空间到生成空间的映射，那么接着就从输入和目标两方面探索数据特性对训练效率的影响。

#### 对数据驱动学习的理论和实践分析
