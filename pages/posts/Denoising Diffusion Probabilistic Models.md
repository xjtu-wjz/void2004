---
title: Paper reading--《Denoising Diffusion Probabilistic Models》
date: 2024-11-03
updated: 2024-11-03
categories: Paper-reading
image: https://raw.githubusercontent.com/xjtu-wjz/void2004/refs/heads/main/pics_for_post/ASurvey%20on%20Multimodal%20Large%20Language%20Models.webp
tags:
  - 科研
  - LLM
top: 1
---

扩散模型的开山之作[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)。这篇帖子并不把重心放在详细的公式推导上，在看见树木之前我们最好先看得见森林。

# 什么是diffusion model?
现在假如我们想要生成一张256x256x3的图片，那么我们需要知道其中每一个像素点需要安放在哪一个位置上。换句话说，我们需要得知像素$x$和其对应位置$p$之间的函数关系$f(x)$。但事实上这个分布函数是非常复杂，或者几乎是不可能获取的。

这时候我们就需要换一种思路，**找到能够满足我们的数据分布的基本似然函数**。这就是diffusion model的最初构想。diffusion model（扩散模型）来源于物理学中热力学扩散的概念，核心有正向扩散和反向扩散两部分，分别对应加噪和去噪的过程。扩散模型的核心思想来源于模拟物质在空间中的扩散过程，只不过在具体任务中“物质”对应的是样本点，“空间”指的是数据空间。

# 正向扩散
为什么要加噪（正向扩散）呢？因为原有的图像数据具有非常复杂的特征，数据分布难以把握，加噪可以简化训练流程，对图像加噪最后形成纯高斯噪声的过程也是一种数据加强的过程，模型更容易学习到鲁棒的图像特征。同时加噪过程也可以看做是一个马尔科夫链，每一步加上去的噪声都是基于当前状态的。在一步步加噪过程中模型的学习量更丰富，训练过程更稳健。

# 反向扩散
反向扩散是正向扩散的逆过程，也可以看做逆马尔科夫链，逐步从当前的数据中去处噪声直到恢复原来的数据分布。每一次去噪都基于当前的状态，在去噪过程中模型就可以有效学习数据的核心特征，并学习如何一步步从完全随机的状态出发，复原得到原始图片或者生成其他的图片。

# 扩散细节
#### 前扩散核与反向扩散核
$q(x_t|x_{t-1})$是前扩散核，表示在已知给定图像$x_{t-1}$的情况下推出新加噪图像$x_{t}$的过渡函数。同样地，反扩散核$p_{\theta}(x_{t-1}|x_t)$表示模型在通过降噪还原图像的时候，已知$x_{t}$的条件下推出$x_{t-1}$分布的过渡函数，$\theta$表示模型学习反向扩散过程分布的参数。

#### 正向扩散的细节
正向扩散可表示为一个马尔科夫链，定义为：
$$q(x_t|x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_tI)$$
$$q(x_{1:T}|x_0) = \prod_{t=1}^T q(x_t|x_{t-1})$$

首先从数据集中获取一张图像$x_0$，模型需要对其累加噪声$T$次获得$x_{1},x_{2},x_{3}......x_{T}$，这里再额外给定中间叠加噪声的高斯分布方差超参数${\beta_{t}\in(0,1)}_{t=1}^{T}$.随着t的增大，x越来越接近纯高斯噪声。每一步更新的xt都可以看做是与原本的$x_{t-1}$有一定关联，但是又在原基础上叠加了一定的噪声。各个时间段的图像为各向同性高斯分布。

在diffusion正向扩散过程中能通过$x_{0}$和$\beta$直接计算$x_{t}$对于快速扩散非常重要。展开$x_{t}$可以得到:
