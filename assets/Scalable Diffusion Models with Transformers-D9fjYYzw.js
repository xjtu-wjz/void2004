import{_ as d}from"./ValaxyMain.vue_vue_type_script_setup_true_lang-D6WepyqT.js";import{b as f,e as p,w as t,f as c,a as u,p as i,r as o,g as e}from"./app-B5BMfZs9.js";const h="/assets/dit-DNHJ1lK8.png",g=e("p",null,"文章提出了经典的diffusion transformer。Dit的核心思想是使用transformer代替原本在扩散模型中负责预测噪声的U-net。使用transformer之后的扩散模型可以有效应对规模更加庞大的数据集，并更加有效利用各种类型的监督信息。",-1),_=e("p",null,"具体结构如图：",-1),b=e("figure",null,[e("img",{src:h,alt:"alt text",loading:"lazy",decoding:"async"})],-1),v=e("p",null,"中间的dit block由vit block经过微调得到。文章一共给出了四种可能得设计形式，分别是：",-1),w=e("ul",null,[e("li",null,[e("p",null,"自适应层数（adaLN），这个设计与GAN和扩散模型一致，通过mlp将条件映射至可以利用的token形式。")]),e("li",null,[e("p",null,"交叉注意力机制，将外部监督条件和image token一同送入交叉注意力模块，但是这种做法会提升计算量。")]),e("li",null,[e("p",null,"in-context conditioning，直接将监督信息例如时间步骤，类别等等拼接成一个新token，然后和原本的image token拼接送入训练。")])],-1),$=e("p",null,"最后采用了自适应层数范式。模型规模越大，patchsize越小，生成效果越好。",-1),P={__name:"Scalable Diffusion Models with Transformers",setup(D,{expose:l}){const s=JSON.parse('{"title":"Paper reading--Scalable Diffusion Models with Transformers","description":"","frontmatter":{"title":"Paper reading--Scalable Diffusion Models with Transformers","date":"2024-12-01","updated":"2023-12-01","excerpt":"Dit问世","categories":"Paper-reading","image":"https://raw.githubusercontent.com/xjtu-wjz/void2004/refs/heads/main/pics_for_post/_2024-11-20%20165837.webp","tags":["科研","Generative Models"],"top":1},"headers":[],"relativePath":"pages/posts/Scalable Diffusion Models with Transformers.md","path":"/home/runner/work/void2004/void2004/pages/posts/Scalable Diffusion Models with Transformers.md","lastUpdated":1736184181000}'),r=u(),n=s.frontmatter||{};return r.meta.frontmatter=Object.assign(r.meta.frontmatter||{},s.frontmatter||{}),i("pageData",s),i("valaxy:frontmatter",n),globalThis.$frontmatter=n,l({frontmatter:{title:"Paper reading--Scalable Diffusion Models with Transformers",date:"2024-12-01",updated:"2023-12-01",excerpt:"Dit问世",categories:"Paper-reading",image:"https://raw.githubusercontent.com/xjtu-wjz/void2004/refs/heads/main/pics_for_post/_2024-11-20%20165837.webp",tags:["科研","Generative Models"],top:1}}),(a,M)=>{const m=d;return f(),p(m,{frontmatter:c(n)},{"main-content-md":t(()=>[g,_,b,v,w,$]),"main-header":t(()=>[o(a.$slots,"main-header")]),"main-header-after":t(()=>[o(a.$slots,"main-header-after")]),"main-nav":t(()=>[o(a.$slots,"main-nav")]),"main-content":t(()=>[o(a.$slots,"main-content")]),"main-content-after":t(()=>[o(a.$slots,"main-content-after")]),"main-nav-before":t(()=>[o(a.$slots,"main-nav-before")]),"main-nav-after":t(()=>[o(a.$slots,"main-nav-after")]),comment:t(()=>[o(a.$slots,"comment")]),footer:t(()=>[o(a.$slots,"footer")]),aside:t(()=>[o(a.$slots,"aside")]),"aside-custom":t(()=>[o(a.$slots,"aside-custom")]),default:t(()=>[o(a.$slots,"default")]),_:3},8,["frontmatter"])}}};export{P as default};
